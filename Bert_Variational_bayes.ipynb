{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFbj8qxf_NRn"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!unzip data.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qzr0pad7_Bl9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(directory):\n",
        "    categories = ['ham', 'spam']\n",
        "    data = []\n",
        "    labels = []\n",
        "    # Iterate through each enron directory\n",
        "    for enron_dir in [d for d in os.listdir(directory) if d.startswith('enron')]:\n",
        "        for label, category in enumerate(categories):\n",
        "            category_dir = os.path.join(directory, enron_dir, category)\n",
        "            for filename in os.listdir(category_dir):\n",
        "                file_path = os.path.join(category_dir, filename)\n",
        "                with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                    text = file.read()\n",
        "                    data.append(text)\n",
        "                    labels.append(label)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "data_directory = 'data'\n",
        "emails, labels = load_data(data_directory)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5c-HvJm_BoK"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "encoded_train_data = []\n",
        "encoded_test_data = []\n",
        "\n",
        "for email_text in X_train:\n",
        "    inputs = tokenizer(email_text, return_tensors=\"pt\", max_length=32, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    email_representation = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    encoded_train_data.append(email_representation)\n",
        "\n",
        "encoded_train_data = np.array(encoded_train_data)\n",
        "\n",
        "print(\"Shape of the encoded train data:\", encoded_train_data.shape)\n",
        "\n",
        "\n",
        "for email_text in X_test:\n",
        "    inputs = tokenizer(email_text, return_tensors=\"pt\", max_length=32, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    email_representation = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    encoded_test_data.append(email_representation)\n",
        "\n",
        "encoded_test_data = np.array(encoded_test_data)\n",
        "\n",
        "print(\"Shape of the encoded test data:\", encoded_test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VPawQKz8Xh7b"
      },
      "outputs": [],
      "source": [
        "train_da = torch.from_numpy(encoded_train_data).view(encoded_train_data.shape[0], -1)\n",
        "test_da = torch.from_numpy(encoded_test_data).view(encoded_test_data.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q_Hsyi_K_Bqg"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_da, torch.tensor(y_train))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_da, torch.tensor(y_test))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "nxcPa_suaz5s"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BayesianNeuralNetworkWithBN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
        "        super(BayesianNeuralNetworkWithBN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.weights_in_hidden_mu = nn.Parameter(torch.randn(input_size, hidden_size))\n",
        "        self.weights_in_hidden_log_var = nn.Parameter(torch.randn(input_size, hidden_size))\n",
        "        self.weights_hidden_out_mu = nn.Parameter(torch.randn(hidden_size, output_size))\n",
        "        self.weights_hidden_out_log_var = nn.Parameter(torch.randn(hidden_size, output_size))\n",
        "        self.bias_hidden_mu = nn.Parameter(torch.randn(hidden_size))\n",
        "        self.bias_hidden_log_var = nn.Parameter(torch.randn(hidden_size))\n",
        "        self.bias_out_mu = nn.Parameter(torch.randn(output_size))\n",
        "        self.bias_out_log_var = nn.Parameter(torch.randn(output_size))\n",
        "\n",
        "        self.bn_input = nn.BatchNorm1d(input_size)\n",
        "        self.bn_hidden = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn_input(x)\n",
        "\n",
        "        weights_in_hidden = self.weights_in_hidden_mu + torch.exp(0.5 * self.weights_in_hidden_log_var) * torch.randn_like(self.weights_in_hidden_log_var)\n",
        "        weights_hidden_out = self.weights_hidden_out_mu + torch.exp(0.5 * self.weights_hidden_out_log_var) * torch.randn_like(self.weights_hidden_out_log_var)\n",
        "        bias_hidden = self.bias_hidden_mu + torch.exp(0.5 * self.bias_hidden_log_var) * torch.randn_like(self.bias_hidden_log_var)\n",
        "        bias_out = self.bias_out_mu + torch.exp(0.5 * self.bias_out_log_var) * torch.randn_like(self.bias_out_log_var)\n",
        "\n",
        "        hidden = torch.tanh(torch.matmul(x, weights_in_hidden) + bias_hidden)\n",
        "\n",
        "        hidden = self.bn_hidden(hidden)\n",
        "\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        output = torch.matmul(hidden, weights_hidden_out) + bias_out\n",
        "        return output, (weights_in_hidden, self.weights_in_hidden_log_var,\n",
        "                        weights_hidden_out, self.weights_hidden_out_log_var,\n",
        "                        bias_hidden, self.bias_hidden_log_var,\n",
        "                        bias_out, self.bias_out_log_var)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = train_da.shape[1]\n",
        "hidden_size = 64\n",
        "output_size = 2  \n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BayesianNeuralNetworkWithBN(input_size, hidden_size, output_size, dropout_rate=0.5).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def compute_kl_div(mu, log_var):\n",
        "    return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwzcRUTXaz8R",
        "outputId": "d5b1e9ae-8423-442d-c43e-322be219c7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Average Loss: 9.40502793933149   Accuracy on test data: 0.827846975088968\n",
            "Epoch [2/20], Average Loss: 6.496457071066751   Accuracy on test data: 0.8852313167259787\n",
            "Epoch [3/20], Average Loss: 4.9913874684986554   Accuracy on test data: 0.9068801897983393\n",
            "Epoch [4/20], Average Loss: 3.82845545097885   Accuracy on test data: 0.9137010676156584\n",
            "Epoch [5/20], Average Loss: 3.018462588920005   Accuracy on test data: 0.923932384341637\n",
            "Epoch [6/20], Average Loss: 2.5432858523666364   Accuracy on test data: 0.9295670225385528\n",
            "Epoch [7/20], Average Loss: 2.3033707068342615   Accuracy on test data: 0.9298635824436536\n",
            "Epoch [8/20], Average Loss: 2.1967540368490797   Accuracy on test data: 0.9289739027283511\n",
            "Epoch [9/20], Average Loss: 2.1646005980060616   Accuracy on test data: 0.9292704626334519\n",
            "Epoch [10/20], Average Loss: 2.1464634034981507   Accuracy on test data: 0.9314946619217082\n",
            "Epoch [11/20], Average Loss: 2.1490673734624193   Accuracy on test data: 0.9297153024911032\n",
            "Epoch [12/20], Average Loss: 2.1566388829040073   Accuracy on test data: 0.9322360616844603\n",
            "Epoch [13/20], Average Loss: 2.1493868216925245   Accuracy on test data: 0.931049822064057\n",
            "Epoch [14/20], Average Loss: 2.1440947191571   Accuracy on test data: 0.9280842230130486\n",
            "Epoch [15/20], Average Loss: 2.147868720803538   Accuracy on test data: 0.9303084223013048\n",
            "Epoch [16/20], Average Loss: 2.1424325133275026   Accuracy on test data: 0.9294187425860023\n",
            "Epoch [17/20], Average Loss: 2.1420609146391936   Accuracy on test data: 0.9337188612099644\n",
            "Epoch [18/20], Average Loss: 2.1367799251529247   Accuracy on test data: 0.9354982206405694\n",
            "Epoch [19/20], Average Loss: 2.13432642813962   Accuracy on test data: 0.9298635824436536\n",
            "Epoch [20/20], Average Loss: 2.1381617010983143   Accuracy on test data: 0.9325326215895611\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train() \n",
        "\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        outputs, params = model(batch_inputs)\n",
        "\n",
        "        weights_in_hidden, log_var_weights_in_hidden, weights_hidden_out, log_var_weights_hidden_out, bias_hidden, log_var_bias_hidden, bias_out, log_var_bias_out = params\n",
        "\n",
        "        nll_loss = F.cross_entropy(outputs, batch_labels)\n",
        "\n",
        "        kl_loss = compute_kl_div(weights_in_hidden, log_var_weights_in_hidden) + \\\n",
        "                  compute_kl_div(weights_hidden_out, log_var_weights_hidden_out) + \\\n",
        "                  compute_kl_div(bias_hidden, log_var_bias_hidden) + \\\n",
        "                  compute_kl_div(bias_out, log_var_bias_out)\n",
        "\n",
        "        loss = nll_loss + 1e-4 * kl_loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_len = 0\n",
        "        right = 0\n",
        "        for batch_test, batch_test_label in test_loader:\n",
        "            batch_test = batch_test.to(device)\n",
        "            batch_test_label = batch_test_label.to(device)\n",
        "\n",
        "            test_outputs, _ = model(batch_test)\n",
        "            predictions = torch.argmax(test_outputs, dim=1)\n",
        "\n",
        "            right += torch.sum(predictions == batch_test_label).item()\n",
        "            test_len += batch_test_label.shape[0]\n",
        "\n",
        "    accuracy = right/test_len\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss}  ', \"Accuracy on test data:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P8FqSHlaz-z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
